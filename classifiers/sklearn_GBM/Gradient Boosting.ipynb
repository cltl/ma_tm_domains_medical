{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level classification with a Gradient-Boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the script can be copied to run in parallel on multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.float_format = '{:20,.5f}'.format\n",
    "sns.set()\n",
    "\n",
    "SEED = random.seed(0)\n",
    "EXPERIMENTS = Path('experiments')\n",
    "SUBSET = 'Train' # 'Test'\n",
    "BASE_MODEL_NAME = 'GBC_POS-PMFT' # GradBoostClassifier model trained on dataset 'raw', with POS tag and \n",
    "                                    # PubMed data-trained FastText as features.    \n",
    "\n",
    "# add required libraries etc\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = pd.read_parquet('data/split/train.parquet') # attach words to the training set\n",
    "\n",
    "# the test set is currently withheld (493 documents). Test using a validation split (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotcoding categorical columns...\n",
      "set()\n",
      "No categorical values found in data\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"e9254439-0a10-49fa-b335-72e14fed0e50\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"e9254439-0a10-49fa-b335-72e14fed0e50\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Finished one-hot coding columns\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Finished one-hot coding columns\"\n",
    "\n",
    "def hotcode(df):\n",
    "    \n",
    "\n",
    "    num_cols = df._get_numeric_data().columns\n",
    "    cat_cols = (set(df.columns) - set(num_cols)) - {'Word'}\n",
    "    \n",
    "    print(cat_cols)\n",
    "    dummies = pd.get_dummies(df[cat_cols])\n",
    "    \n",
    "    print('hotcode complete.')\n",
    "    # assert check that type is numeric for all\n",
    "    \n",
    "    return pd.concat([dummies, df[num_cols]], axis=1)\n",
    "\n",
    "print('hotcoding categorical columns...')\n",
    "try:\n",
    "    train_val = hotcode(train_val)\n",
    "except ValueError:\n",
    "    print('No categorical values found in data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Parameters & Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation split is only an example for K-fold crossvalidation to simulate an 80:20 train/validation dataset. See code examples of  https://scikit-learn.org/stable/modules/cross_validation.html and https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 5 # not implemented\n",
    "\n",
    "pio = {\"participants\", \"interventions\", \"outcomes\"}\n",
    "features = set(train_val.columns).difference(pio.union({'Word'}))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f_score', 'support']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Validation split (KCV currently not implemented, so just a simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting data 3600:900\n"
     ]
    }
   ],
   "source": [
    "train_idx, val_idx = train_test_split(train_val.index.unique('doc'), \n",
    "                                      train_size=1-(1/k_folds)) # default 70:30\n",
    "\n",
    "print(f'splitting data {len(train_idx)}:{len(val_idx)}')\n",
    "\n",
    "train_set = train_val.loc[(train_idx, slice(None)),:]\n",
    "val_set   = train_val.loc[(val_idx, slice(None)),:]\n",
    "\n",
    "X_train, X_test = train_set[features], val_set[features]\n",
    "y_train, y_test = train_set[pio.union({'Word'})], val_set[pio.union({'Word'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evluating model on validation set(s) of size 248493 with 900 documents\n",
      "Prepared dataset has 415 features.\n"
     ]
    }
   ],
   "source": [
    "# convert labels to string (pd.Categorical?)\n",
    "y_train, y_test = [pd.concat([y[key].astype('str') for key in y],axis=1) for y in [y_train, y_test]]\n",
    "\n",
    "print(f\"Evluating model on validation set(s) of size {len(val_set)} with {len(val_set.index.unique('doc'))} documents\")\n",
    "print(f'Prepared dataset has {len(features)} features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Train a model for each feature. Can also try a multiclass model, since only 3% of the labels overlap.\n",
    "\n",
    "The model trains n_estimators, which are binary decision trees on a single feature. 'Boosting' refers to the fact that the trees learn to choose the most informative variables over generations, meaning the score gradually improves. The aggregate label prediction for each word is taken over all estimators. There are many tunable parameters for this model but for initial testing I've used 300 estimators and 416 features (meaning not all features will be explored, see the feature importance results).\n",
    "\n",
    "The class weights are set with parameter `weight_factor`. If ` weight_factor=5`, then the class weight for the positive label is five times that of the negative label.\n",
    "\n",
    "Highly advisable to do a test run with >3 estimators to see if all the results are saved correctly before running with a long training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name set to GBM_w3\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_NAME = 'GBM'\n",
    "time = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "dir_name = BASE_MODEL_NAME  + f'_{time}'\n",
    "\n",
    "# manually override the directory name. Careful, as you won't be warned if you are overwriting existing data.\n",
    "# this is useful for running several notebooks in parallel (multi-processor)\n",
    "\n",
    "dir_name = 'GBM_w3'\n",
    "\n",
    "print(f'Folder name set to {dir_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['participants','interventions','outcomes']\n",
    "n_estimators = 300\n",
    "n_runs = 1\n",
    "weight_factor = 3 # weight factor of positive class compared to negative\n",
    "\n",
    "words = pd.read_pickle('data\\\\raw\\\\labels.pkl')\n",
    "\n",
    "assert \"Word\" not in features\n",
    "assert np.array([(label not in features) for label in pio]).all()\n",
    "\n",
    "# use these overrides for multiple scripts\n",
    "targets = ['participants']\n",
    "# targets = ['interventions']\n",
    "# targets = ['outcomes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"388f21e6-83b0-4d2c-ab62-e82a131a93a3\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"388f21e6-83b0-4d2c-ab62-e82a131a93a3\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"fStarting new iteration for participants\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GradientBoosting model to predict participants on 3600 training instances for label participants\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.1974          101.18m\n",
      "         2           1.1786          102.45m\n",
      "         3           1.1659           99.80m\n",
      "         4           1.1559           98.11m\n",
      "         5           1.1464           97.37m\n",
      "         6           1.1385           96.24m\n",
      "         7           1.1307           97.42m\n",
      "         8           1.1252           98.40m\n",
      "         9           1.1202          100.07m\n",
      "        10           1.1152          101.55m\n",
      "        20           1.0796           91.30m\n",
      "        30           1.0606           83.62m\n",
      "        40           1.0481           78.97m\n",
      "        50           1.0395           75.19m\n",
      "        60           1.0318           71.46m\n",
      "        70           1.0246           67.83m\n",
      "        80           1.0191           64.58m\n",
      "        90           1.0144           61.32m\n",
      "       100           1.0104           58.21m\n",
      "       200           0.9885           28.67m\n",
      "       300           0.9778            0.00s\n",
      "Saving data for run 0, target participants in GBM_w3\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "for run in range(n_runs):\n",
    "    \n",
    "    # cross-validation parameters defined here for now. Use sklearn.GridSearchCV for param tuning\n",
    "\n",
    "    train_weights = y_train[pio].astype(int) * (1 - 1/weight_factor) + 1 / weight_factor\n",
    "    test_weights = y_test[pio].astype(int) * (1 - 1/weight_factor) + 1 / weight_factor\n",
    "        \n",
    "    for target in targets:\n",
    "        \n",
    "        %notify -m f\"Starting new iteration for {target}\"    \n",
    "            \n",
    "        print(f\"Training GradientBoosting model to predict {target} \"\n",
    "              f\"on {len(X_train.index.unique('doc'))} training instances \" \n",
    "              f\"for label {target}\")\n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=1.0, \n",
    "                                         verbose=1, max_depth=1, random_state=0)\n",
    "              \n",
    "        clf = clf.fit(X_train, y_train[target], sample_weight=train_weights[target].values)\n",
    "              \n",
    "        score = clf.score(X_test, y_test[target], sample_weight=test_weights[target].values)\n",
    "              \n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_true = y_test[target].values \n",
    "              \n",
    "        print(f'Saving data for run {run}, target {target} in {dir_name}')\n",
    "              \n",
    "        model_name = target[0].upper() + f'_wf{weight_factor}_run{run}'\n",
    "        save_model = True\n",
    "        \n",
    "        if save_model:\n",
    "            Path(f'models/{dir_name}').mkdir(exist_ok=True, parents=True)\n",
    "            dump(clf, f'models/{dir_name}/{model_name}.joblib')\n",
    "        \n",
    "        exp_folder = EXPERIMENTS / dir_name / model_name\n",
    "        exp_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        feat_imp = pd.DataFrame(clf.feature_importances_, index=X_test.columns)\n",
    "        train_loss = pd.DataFrame(clf.train_score_)\n",
    "        feat_imp.sort_index().to_csv(exp_folder /  'feature_importance.csv')\n",
    "        train_loss.to_csv(exp_folder / 'training_loss.csv')\n",
    "        \n",
    "        pred_results = pd.DataFrame([y_test['Word'].values, y_true, y_pred], \n",
    "                                    index=['Word', 'T', 'P']).T\n",
    "        pred_results.to_csv(exp_folder / 'predictions.csv')\n",
    "              \n",
    "        results = precision_recall_fscore_support(y_true, y_pred, sample_weight=test_weights[target])\n",
    "        results = pd.DataFrame.from_dict(results)\n",
    "        results.index = metrics[1:]\n",
    "        results.to_csv(exp_folder / 'results.csv')\n",
    "        \n",
    "        with (exp_folder / 'accuracy.txt').open('a') as f:\n",
    "            f.write(f'{accuracy_score(y_true, y_pred, sample_weight=test_weights[target])}')\n",
    "            f.write('\\n')\n",
    "            f.write(f'{features}')\n",
    "            f.close()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "important aspects still to be implemented:\n",
    "- parameter tuning w/ grid search (train short models on subsets? try param_grid and GridSearch)\n",
    "- KCV (in combination with above)\n",
    "- some testing with different preprocessing and feature sets.\n",
    "- try importing seaborn and doing some EDA on feature importance data?\n",
    "- test Random Forest version?\n",
    "- additional feature extraction and tagging (dep relationships and sentence level features)\n",
    "- If trees are interesting as a final model, XGBoost > sklearn on most problems."
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
