{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from processing.corpus import Corpus\n",
    "\n",
    "SEED = np.random.seed(0)\n",
    "DATA = Path('data')\n",
    "TARGETS = ['participants', 'interventions', 'outcomes']\n",
    "SUBSET = 'Train' # 'Test'\n",
    "N_TRAIN_DOCS = 4000\n",
    "SAVE_LOC = Path('data\\\\split')\n",
    "\n",
    "SAVE_LOC.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Load feature set and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data load complete.\n"
     ]
    }
   ],
   "source": [
    "labels_path = DATA / 'raw' / 'labels.parquet'\n",
    "feats_in = pd.read_parquet(DATA / 'dataset3.parquet')\n",
    "labels_in = pd.read_parquet(labels_path).drop('Word', axis=1)\n",
    "      \n",
    "df = pd.concat([c.reset_index(drop=True) for c in [feats_in, labels_in]], axis=1).set_index(feats_in.index)\n",
    "\n",
    "assert not df.columns.duplicated().any()\n",
    "      \n",
    "print('Data load complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample = True\n",
    "# if downsample:\n",
    "#     n = 2500\n",
    "#     doc_ids = list(df.index.unique('doc'))\n",
    "#     sample_idx = random.sample(doc_ids, n)\n",
    "\n",
    "#     df = df.loc[(sample_idx, slice(None), slice(None)), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Train/Test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Withhold part of the data for evaluation. Could update using the same method as in GBC script, as it seems to be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting data 4493:500\n"
     ]
    }
   ],
   "source": [
    "k_folds = 10 # only simulated for now\n",
    "\n",
    "train_idx, val_idx = train_test_split(df.index.unique('doc'), train_size=1-(1/k_folds)) # default 80:20\n",
    "\n",
    "print(f'splitting data {len(train_idx)}:{len(val_idx)}')\n",
    "\n",
    "train_val = df.loc[(train_idx, slice(None)),:]\n",
    "test_hold = df.loc[(val_idx, slice(None)),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_ids = list(df.index.unique(level='doc'))\n",
    "n_docs = [4000]\n",
    "\n",
    "train_ds = [train_val.loc[random.sample(doc_ids, n)] for n in n_docs]\n",
    "\n",
    "\n",
    "train_val.to_parquet(SAVE_LOC / 'train.parquet')\n",
    "test_hold.to_parquet(SAVE_LOC / 'test.parquet')\n",
    "            \n",
    "# for i, data in enumerate(train_ds):\n",
    "#     data.to_parquet(SAVE_LOC / f'train_{n_docs[i]}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc       sent  word\n",
       "10036953  1     1               _\n",
       "                2               _\n",
       "                3         therapy\n",
       "                4         regimen\n",
       "                5               _\n",
       "                          ...    \n",
       "9989713   15    21      randomise\n",
       "                22              _\n",
       "                23       clinical\n",
       "                24          trial\n",
       "                25              _\n",
       "Name: lemma, Length: 1219236, dtype: category\n",
       "Categories (899, object): [30, 60, _, abdominal, ..., xDECx, xNUMx, year, young]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_val['lemma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
