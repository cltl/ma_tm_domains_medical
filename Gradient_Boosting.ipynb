{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level classification with a Gradient-Boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the script can be copied to run in parallel on multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.float_format = '{:20,.5f}'.format\n",
    "sns.set()\n",
    "\n",
    "SEED = random.seed(0)\n",
    "EXPERIMENTS = Path('experiments')\n",
    "SUBSET = 'Train' # 'Test'\n",
    "BASE_MODEL_NAME = 'GBC_POS-PMFT' # GradBoostClassifier model trained on dataset 'raw', with POS tag and \n",
    "                                    # PubMed data-trained FastText as features.    \n",
    "\n",
    "# add required libraries etc\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_val = pd.read_parquet('data/split/train.parquet')\n",
    "train_val = pd.read_parquet('data/features/km_4-8-12_10x300.parquet')\n",
    "\n",
    "# the test set is currently withheld (493 documents). Test using a validation split (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotcoding categorical columns...\n",
      "set()\n",
      "No categorical values found in data\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"ecb56ad3-24c0-4464-8937-6f003d363f04\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"ecb56ad3-24c0-4464-8937-6f003d363f04\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Finished one-hot coding columns\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify -m \"Finished one-hot coding columns\"\n",
    "\n",
    "def hotcode(df):\n",
    "    \n",
    "\n",
    "    num_cols = df._get_numeric_data().columns\n",
    "    cat_cols = (set(df.columns) - set(num_cols)) - {'Word'}\n",
    "    \n",
    "    print(cat_cols)\n",
    "    dummies = pd.get_dummies(df[cat_cols])\n",
    "    \n",
    "    print('hotcode complete.')\n",
    "    # assert check that type is numeric for all\n",
    "    \n",
    "    return pd.concat([dummies, df[num_cols]], axis=1)\n",
    "\n",
    "print('hotcoding categorical columns...')\n",
    "try:\n",
    "    train_val = hotcode(train_val)\n",
    "except ValueError:\n",
    "    print('No categorical values found in data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Parameters & Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation split is only an example for K-fold crossvalidation to simulate an 80:20 train/validation dataset. See code examples of  https://scikit-learn.org/stable/modules/cross_validation.html and https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 5 # not implemented\n",
    "\n",
    "pio = {\"participants\", \"interventions\", \"outcomes\"}\n",
    "features = set(train_val.columns).difference(pio.union({'Word'}))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f_score', 'support']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Validation split (KCV currently not implemented, so just a simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting data 3600:900\n"
     ]
    }
   ],
   "source": [
    "train_idx, val_idx = train_test_split(train_val.index.unique('doc'), \n",
    "                                      train_size=1-(1/k_folds)) # default 70:30\n",
    "\n",
    "print(f'splitting data {len(train_idx)}:{len(val_idx)}')\n",
    "\n",
    "train_set = train_val.loc[(train_idx, slice(None)),:]\n",
    "val_set   = train_val.loc[(val_idx, slice(None)),:]\n",
    "\n",
    "X_train, X_test = train_set[features], val_set[features]\n",
    "y_train, y_test = train_set[pio.union({'Word'})], val_set[pio.union({'Word'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evluating model on validation set(s) of size 243684 with 900 documents\n",
      "Prepared dataset has 120 features.\n"
     ]
    }
   ],
   "source": [
    "# convert labels to string (pd.Categorical?)\n",
    "y_train, y_test = [pd.concat([y[key].astype('str') for key in y],axis=1) for y in [y_train, y_test]]\n",
    "\n",
    "print(f\"Evluating model on validation set(s) of size {len(val_set)} with {len(val_set.index.unique('doc'))} documents\")\n",
    "print(f'Prepared dataset has {len(features)} features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Train a model for each feature. Can also try a multiclass model, since only 3% of the labels overlap.\n",
    "\n",
    "The model trains n_estimators, which are binary decision trees on a single feature. 'Boosting' refers to the fact that the trees learn to choose the most informative variables over generations, meaning the score gradually improves. The aggregate label prediction for each word is taken over all estimators. There are many tunable parameters for this model but for initial testing I've used 300 estimators and 416 features (meaning not all features will be explored, see the feature importance results).\n",
    "\n",
    "The class weights are set with parameter `weight_factor`. If ` weight_factor=5`, then the class weight for the positive label is five times that of the negative label.\n",
    "\n",
    "Highly advisable to do a test run with >3 estimators to see if all the results are saved correctly before running with a long training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name set to kM_clusttree_4-8-12\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_NAME = 'GBM'\n",
    "time = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "dir_name = BASE_MODEL_NAME  + f'_{time}'\n",
    "\n",
    "# manually override the directory name. Careful, as you won't be warned if you are overwriting existing data.\n",
    "# this is useful for running several notebooks in parallel (multi-processor)\n",
    "\n",
    "dir_name = 'kM_clusttree_4-8-12'\n",
    "\n",
    "print(f'Folder name set to {dir_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['participants','interventions','outcomes']\n",
    "n_estimators = 300\n",
    "n_runs = 10\n",
    "weight_factor = 5 # weight factor of positive class compared to negative\n",
    "\n",
    "words = pd.read_pickle('data\\\\raw\\\\labels.pkl')\n",
    "\n",
    "assert \"Word\" not in features\n",
    "assert np.array([(label not in features) for label in pio]).all()\n",
    "\n",
    "# use these overrides for multiple scripts\n",
    "# targets = ['participants']\n",
    "# targets = ['interventions']\n",
    "# targets = ['outcomes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"61b9da25-df3e-4c8d-8a1d-62ccbce0a45f\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"61b9da25-df3e-4c8d-8a1d-62ccbce0a45f\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"fStarting new iteration for participants\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GradientBoosting model to predict participants on 3600 training instances for label participants\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3088            8.25m\n",
      "         2           1.2746            8.28m\n",
      "         3           1.2456            8.10m\n",
      "         4           1.2259            7.96m\n",
      "         5           1.2128            7.87m\n",
      "         6           1.2052            7.88m\n",
      "         7           1.1999            7.91m\n",
      "         8           1.1941            7.87m\n",
      "         9           1.1897            7.86m\n",
      "        10           1.1862            7.85m\n",
      "        20           1.1607            7.67m\n",
      "        30           1.1498            7.56m\n",
      "        40           1.1437            7.18m\n",
      "        50           1.1398            6.84m\n",
      "        60           1.1373            6.61m\n",
      "        70           1.1359            6.88m\n",
      "        80           1.1348            6.91m\n",
      "        90           1.1338            6.91m\n",
      "       100           1.1330            6.75m\n"
     ]
    }
   ],
   "source": [
    "for run in range(n_runs):\n",
    "    \n",
    "    # cross-validation parameters defined here for now. Use sklearn.GridSearchCV for param tuning\n",
    "\n",
    "    train_weights = y_train[pio].astype(int) * (1 - 1/weight_factor) + 1 / weight_factor\n",
    "    test_weights = y_test[pio].astype(int) * (1 - 1/weight_factor) + 1 / weight_factor\n",
    "        \n",
    "    for target in targets:\n",
    "        \n",
    "        %notify -m f\"Starting new iteration for {target}\"    \n",
    "            \n",
    "        print(f\"Training GradientBoosting model to predict {target} \"\n",
    "              f\"on {len(X_train.index.unique('doc'))} training instances \" \n",
    "              f\"for label {target}\")\n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=1.0, \n",
    "                                         verbose=1, max_depth=1, random_state=0)\n",
    "              \n",
    "        clf = clf.fit(X_train, y_train[target], sample_weight=train_weights[target].values)\n",
    "              \n",
    "        score = clf.score(X_test, y_test[target], sample_weight=test_weights[target].values)\n",
    "              \n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_true = y_test[target].values \n",
    "              \n",
    "        print(f'Saving data for run {run}, target {target} in {dir_name}')\n",
    "              \n",
    "        model_name = target[0].upper() + f'_wf{weight_factor}_run{run}'\n",
    "        save_model = True\n",
    "        \n",
    "        if save_model:\n",
    "            Path(f'models/{dir_name}').mkdir(exist_ok=True, parents=True)\n",
    "            dump(clf, f'models/{dir_name}/{model_name}.joblib')\n",
    "        \n",
    "        exp_folder = EXPERIMENTS / dir_name / model_name\n",
    "        exp_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        feat_imp = pd.DataFrame(clf.feature_importances_, index=X_test.columns)\n",
    "        train_loss = pd.DataFrame(clf.train_score_)\n",
    "        feat_imp.sort_index().to_csv(exp_folder /  'feature_importance.csv')\n",
    "        train_loss.to_csv(exp_folder / 'training_loss.csv')\n",
    "        \n",
    "        pred_results = pd.DataFrame([y_test['Word'].values, y_true, y_pred], \n",
    "                                    index=['Word', 'T', 'P']).T\n",
    "        pred_results.to_csv(exp_folder / 'predictions.csv')\n",
    "              \n",
    "        results = precision_recall_fscore_support(y_true, y_pred, sample_weight=test_weights[target])\n",
    "        results = pd.DataFrame.from_dict(results)\n",
    "        results.index = metrics[1:]\n",
    "        results.to_csv(exp_folder / 'results.csv')\n",
    "        \n",
    "        with (exp_folder / 'accuracy.txt').open('a') as f:\n",
    "            f.write(f'{accuracy_score(y_true, y_pred, sample_weight=test_weights[target])}')\n",
    "            f.write('\\n')\n",
    "            f.write(f'{features}')\n",
    "            f.close()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "important aspects still to be implemented:\n",
    "- parameter tuning w/ grid search (train short models on subsets? try param_grid and GridSearch)\n",
    "- KCV (in combination with above)\n",
    "- some testing with different preprocessing and feature sets.\n",
    "- try importing seaborn and doing some EDA on feature importance data?\n",
    "- test Random Forest version?\n",
    "- additional feature extraction and tagging (dep relationships and sentence level features)\n",
    "- If trees are interesting as a final model, XGBoost > sklearn on most problems."
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
