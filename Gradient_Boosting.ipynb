{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-level classification with a Gradient-Boosting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: the script can be copied to run in parallel on multiple cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.float_format = '{:20,.5f}'.format\n",
    "sns.set()\n",
    "\n",
    "SEED = random.seed(0)\n",
    "EXPERIMENTS = Path('experiments')\n",
    "TARGETS = ['participants','interventions','outcomes']\n",
    "SUBSET = 'Train' # 'Test'\n",
    "BASE_MODEL_NAME = 'GBC_POS-PMFT' # GradBoostClassifier model trained on dataset 'raw', with POS tag and \n",
    "                                    # PubMed data-trained FastText as features.    \n",
    "\n",
    "# add required libraries etc\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = pd.read_parquet('data/split/train.parquet') # attach words to the training set\n",
    "\n",
    "# the test set is currently withheld (493 documents). Test using a validation split (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotcoding categorical columns...\n",
      "set()\n",
      "No categorical values found in data\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"5a1941a8-9e05-4406-8b0b-9ad569d9042b\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"5a1941a8-9e05-4406-8b0b-9ad569d9042b\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "\n",
    "def hotcode(df):\n",
    "    \n",
    "\n",
    "    num_cols = df._get_numeric_data().columns\n",
    "    cat_cols = (set(df.columns) - set(num_cols)) - {'Word'}\n",
    "    \n",
    "    print(cat_cols)\n",
    "    dummies = pd.get_dummies(df[cat_cols])\n",
    "    \n",
    "    print('hotcode complete.')\n",
    "    # assert check that type is numeric for all\n",
    "    \n",
    "    return pd.concat([dummies, df[num_cols]], axis=1)\n",
    "\n",
    "print('hotcoding categorical columns...')\n",
    "try:\n",
    "    train_val = hotcode(train_val)\n",
    "except ValueError:\n",
    "    print('No categorical values found in data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "# Parameters & Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and validation split is only an example for K-fold crossvalidation to simulate an 80:20 train/validation dataset. See code examples of  https://scikit-learn.org/stable/modules/cross_validation.html and https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 5 # not implemented\n",
    "\n",
    "pio = {\"participants\", \"interventions\", \"outcomes\"}\n",
    "features = set(train_val.columns).difference(pio.union({'Word'}))\n",
    "\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f_score', 'support']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Validation split (KCV currently not implemented, so just a simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting data 3600:900\n"
     ]
    }
   ],
   "source": [
    "train_idx, val_idx = train_test_split(train_val.index.unique('doc'), \n",
    "                                      train_size=1-(1/k_folds)) # default 70:30\n",
    "\n",
    "print(f'splitting data {len(train_idx)}:{len(val_idx)}')\n",
    "\n",
    "train_set = train_val.loc[(train_idx, slice(None)),:]\n",
    "val_set   = train_val.loc[(val_idx, slice(None)),:]\n",
    "\n",
    "X_train, X_test = train_set[features], val_set[features]\n",
    "y_train, y_test = train_set[pio.union({'Word'})], val_set[pio.union({'Word'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evluating model on validation set(s) of size 249361 with 900 documents\n",
      "Prepared dataset has 415 features.\n"
     ]
    }
   ],
   "source": [
    "# convert labels to string (pd.Categorical?)\n",
    "y_train, y_test = [pd.concat([y[key].astype('str') for key in y],axis=1) for y in [y_train, y_test]]\n",
    "\n",
    "print(f\"Evluating model on validation set(s) of size {len(val_set)} with {len(val_set.index.unique('doc'))} documents\")\n",
    "print(f'Prepared dataset has {len(features)} features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "Train a model for each feature. Can also try a multiclass model, since only 3% of the labels overlap.\n",
    "\n",
    "The model trains n_estimators, which are binary decision trees on a single feature. 'Boosting' refers to the fact that the trees learn to choose the most informative variables over generations, meaning the score gradually improves. The aggregate label prediction for each word is taken over all estimators. There are many tunable parameters for this model but for initial testing I've used 300 estimators and 416 features (meaning not all features will be explored, see the feature importance results).\n",
    "\n",
    "The class weights are set with parameter `weight_factor`. If ` weight_factor=5`, then the class weight for the positive label is five times that of the negative label.\n",
    "\n",
    "Highly advisable to do a test run with >3 estimators to see if all the results are saved correctly before running with a long training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder name set to GBM_0309-2105\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL_NAME = 'GBM'\n",
    "time = datetime.now().strftime(\"%m%d-%H%M\")\n",
    "dir_name = BASE_MODEL_NAME  + f'_{time}'\n",
    "\n",
    "# manually override the directory name. Careful, as you will not be warned if you are overwriting existing data.\n",
    "# this is useful for running several notebooks in parallel (multi-processor)\n",
    "\n",
    "# dir_name = 'GM'\n",
    "\n",
    "print(f'Folder name set to {dir_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = TARGETS # choose which variables you want to predict\n",
    "n_estimators = 300\n",
    "n_runs = 1\n",
    "weight_factor = 5 # weight factor of positive class compared to negative\n",
    "\n",
    "words = pd.read_pickle('data\\\\raw\\\\labels.pkl')\n",
    "\n",
    "assert \"Word\" not in features\n",
    "assert np.array([(label not in features) for label in pio]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training GradientBoosting model to predict participants on 3600 training instances for label participants\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3234           69.03m\n",
      "         2           1.3016           67.57m\n",
      "         3           1.2867           65.78m\n",
      "         4           1.2751           64.62m\n",
      "         5           1.2649           63.94m\n",
      "         6           1.2567           64.00m\n",
      "         7           1.2503           63.51m\n",
      "         8           1.2446           63.51m\n",
      "         9           1.2395           63.82m\n",
      "        10           1.2338           63.62m\n",
      "        20           1.1956           61.84m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-909c6eae4b92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                                          verbose=1, max_depth=1, random_state=0)\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_weights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1535\u001b[0m         n_stages = self._fit_stages(\n\u001b[0;32m   1536\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1537\u001b[1;33m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1539\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1592\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[0;32m   1593\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1594\u001b[1;33m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1596\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m-> 1245\u001b[1;33m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1247\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1225\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1226\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"30171bdd-62e1-42a4-88de-0cbb0a175936\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"30171bdd-62e1-42a4-88de-0cbb0a175936\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%notify\n",
    "\n",
    "# dump params to file\n",
    "\n",
    "for run in range(n_runs):\n",
    "    \n",
    "    # cross-validation parameters defined here for now. Use sklearn.GridSearchCV for param tuning\n",
    "\n",
    "    train_weights = y_train[pio].astype(int) * (1 - 1/weight_factor) + 1 / weight_factor\n",
    "    test_weights = y_test[pio].astype(int) * (1 - 1/weight_factor) + 1 / weight_factor\n",
    "        \n",
    "    for target in TARGETS:\n",
    "        \n",
    "        %%notify -m \"Starting new iteration\"    \n",
    "            \n",
    "        print(f\"Training GradientBoosting model to predict {target} \"\n",
    "              f\"on {len(X_train.index.unique('doc'))} training instances \" \n",
    "              f\"for label {target}\")\n",
    "\n",
    "        clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=1.0, \n",
    "                                         verbose=1, max_depth=1, random_state=0)\n",
    "              \n",
    "        clf = clf.fit(X_train, y_train[target], sample_weight=train_weights[target].values)\n",
    "              \n",
    "        score = clf.score(X_test, y_test[target], sample_weight=test_weights[target].values)\n",
    "              \n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_true = y_test[target].values \n",
    "              \n",
    "        print(f'Saving data for run {run}, target {target} in {dir_name}')\n",
    "              \n",
    "        model_name = target[0].upper() + f'_wf{weight_factor}_run{run}'\n",
    "        save_model = True\n",
    "        \n",
    "        if save_model:\n",
    "            Path(f'models/{dir_name}').mkdir(exist_ok=True, parents=True)\n",
    "            dump(clf, f'models/{dir_name}/{model_name}.joblib')\n",
    "        \n",
    "        exp_folder = EXPERIMENTS / dir_name / model_name\n",
    "        exp_folder.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        feat_imp = pd.DataFrame(clf.feature_importances_, index=X_test.columns)\n",
    "        train_loss = pd.DataFrame(clf.train_score_)\n",
    "        feat_imp.sort_index().to_csv(exp_folder /  'feature_importance.csv')\n",
    "        train_loss.to_csv(exp_folder / 'training_loss.csv')\n",
    "        \n",
    "        pred_results = pd.DataFrame([y_test['Word'].values, y_true, y_pred], \n",
    "                                    index=['Word', 'T', 'P']).T\n",
    "        pred_results.to_csv(exp_folder / 'predictions.csv')\n",
    "              \n",
    "        results = precision_recall_fscore_support(y_true, y_pred, sample_weight=test_weights[target])\n",
    "        results = pd.DataFrame.from_dict(results)\n",
    "        results.index = metrics[1:]\n",
    "        results.to_csv(exp_folder / 'results.csv')\n",
    "        \n",
    "        with (exp_folder / 'accuracy.txt').open('a') as f:\n",
    "            f.write(f'{accuracy_score(y_true, y_pred, sample_weight=test_weights[target])}')\n",
    "            f.write('\\n')\n",
    "            f.write(f'{features}')\n",
    "            f.close()\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "important aspects still to be implemented:\n",
    "- parameter tuning w/ grid search (train short models on subsets? try param_grid and GridSearch)\n",
    "- KCV (in combination with above)\n",
    "- some testing with different preprocessing and feature sets.\n",
    "- try importing seaborn and doing some EDA on feature importance data?\n",
    "- test Random Forest version?\n",
    "- additional feature extraction and tagging (dep relationships and sentence level features)\n",
    "- If trees are interesting as a final model, XGBoost > sklearn on most problems."
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
